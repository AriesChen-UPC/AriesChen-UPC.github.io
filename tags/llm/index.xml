<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>LLM - Tag - AriesChen&#39;s Blog</title>
        <link>http://localhost:1313/tags/llm/</link>
        <description>LLM - Tag - AriesChen&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="http://localhost:1313/tags/llm/" rel="self" type="application/rss+xml" /><item>
    <title>大语言模型架构中的注意力机制</title>
    <link>http://localhost:1313/post/docs/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</link>
    <pubDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</pubDate><author>
        <name>AriesChen</name>
    </author><guid>http://localhost:1313/post/docs/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</guid>
    <description><![CDATA[I. 引言：注意力在序列模型和大型语言模型中的作用 A. 传统序列模型（RNN/LSTM）的局限性在深度学习应用于自然语言处理（NLP）的早期阶段，循]]></description>
</item></channel>
</rss>
