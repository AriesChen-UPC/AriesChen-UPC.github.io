<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>ChatGPT - Tag - AriesChen&#39;s Blog</title>
        <link>http://localhost:1313/tags/chatgpt/</link>
        <description>ChatGPT - Tag - AriesChen&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="http://localhost:1313/tags/chatgpt/" rel="self" type="application/rss+xml" /><item>
    <title>大语言模型架构中的注意力机制</title>
    <link>http://localhost:1313/post/docs/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</link>
    <pubDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</pubDate><author>
        <name>AriesChen</name>
    </author><guid>http://localhost:1313/post/docs/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</guid>
    <description><![CDATA[简化的自注意力在深入探讨大型语言模型中使用的复杂注意力机制之前，理解一个简化的自注意力（Simplified Self-Attention）版]]></description>
</item><item>
    <title>深度学习中的注意力机制</title>
    <link>http://localhost:1313/post/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</link>
    <pubDate>Tue, 20 Dec 2022 00:00:00 &#43;0000</pubDate><author>
        <name>AriesChen</name>
    </author><guid>http://localhost:1313/post/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</guid>
    <description><![CDATA[深度学习中的注意力机制注意力机制（Attention Mechanism）是深度学习领域的一项关键技术，它使模型能够在处理输入数据时，选择性地]]></description>
</item><item>
    <title>Transformer 架构深度解析</title>
    <link>http://localhost:1313/post/docs/transformer-%E6%9E%B6%E6%9E%84%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/</link>
    <pubDate>Wed, 30 Nov 2022 00:00:00 &#43;0000</pubDate><author>
        <name>AriesChen</name>
    </author><guid>http://localhost:1313/post/docs/transformer-%E6%9E%B6%E6%9E%84%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/</guid>
    <description><![CDATA[Transformer架构的诞生背景与核心目标序列到序列（Sequence-to-Sequence, Seq2Seq）模型旨在解决将一个输入序]]></description>
</item></channel>
</rss>
