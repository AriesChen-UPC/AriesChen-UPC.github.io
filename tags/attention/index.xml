<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Attention - Tag - AriesChen&#39;s Blog</title>
        <link>http://localhost:1313/tags/attention/</link>
        <description>Attention - Tag - AriesChen&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="http://localhost:1313/tags/attention/" rel="self" type="application/rss+xml" /><item>
    <title>大语言模型架构中的注意力机制</title>
    <link>http://localhost:1313/post/docs/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</link>
    <pubDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</pubDate><author>
        <name>AriesChen</name>
    </author><guid>http://localhost:1313/post/docs/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</guid>
    <description><![CDATA[I. 引言：注意力在序列模型和大型语言模型中的作用 A. 传统序列模型（RNN/LSTM）的局限性在深度学习应用于自然语言处理（NLP）的早期阶段，循]]></description>
</item><item>
    <title>自注意力机制中的查询、键和值</title>
    <link>http://localhost:1313/post/docs/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%B8%AD%E7%9A%84%E6%9F%A5%E8%AF%A2%E9%94%AE%E5%92%8C%E5%80%BC/</link>
    <pubDate>Sun, 05 Feb 2023 00:00:00 &#43;0000</pubDate><author>
        <name>AriesChen</name>
    </author><guid>http://localhost:1313/post/docs/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%B8%AD%E7%9A%84%E6%9F%A5%E8%AF%A2%E9%94%AE%E5%92%8C%E5%80%BC/</guid>
    <description><![CDATA[1. 引言：序列建模的挑战与注意力的兴起序列数据，如自然语言文本、时间序列信号等，在人工智能领域无处不在。有效处理和理解这些序列是许多任务（如机]]></description>
</item><item>
    <title>深度学习中的注意力机制</title>
    <link>http://localhost:1313/post/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</link>
    <pubDate>Tue, 20 Dec 2022 00:00:00 &#43;0000</pubDate><author>
        <name>AriesChen</name>
    </author><guid>http://localhost:1313/post/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</guid>
    <description><![CDATA[I. 深度学习中的注意力机制导论 A. 定义注意力：核心概念与目的注意力机制（Attention Mechanism）是深度学习领域的一项关键技术，它使]]></description>
</item></channel>
</rss>
