<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Attention - Tag - AriesChen&#39;s Blog</title>
        <link>http://localhost:1313/tags/attention/</link>
        <description>Attention - Tag - AriesChen&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="http://localhost:1313/tags/attention/" rel="self" type="application/rss+xml" /><item>
    <title>大语言模型架构中的注意力机制</title>
    <link>http://localhost:1313/post/docs/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</link>
    <pubDate>Mon, 06 Mar 2023 00:00:00 &#43;0000</pubDate><author>
        <name>AriesChen</name>
    </author><guid>http://localhost:1313/post/docs/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</guid>
    <description><![CDATA[简化的自注意力在深入探讨大型语言模型中使用的复杂注意力机制之前，理解一个简化的自注意力（Simplified Self-Attention）版]]></description>
</item><item>
    <title>自注意力机制中的查询、键和值</title>
    <link>http://localhost:1313/post/docs/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%B8%AD%E7%9A%84%E6%9F%A5%E8%AF%A2%E9%94%AE%E5%92%8C%E5%80%BC/</link>
    <pubDate>Sun, 05 Feb 2023 00:00:00 &#43;0000</pubDate><author>
        <name>AriesChen</name>
    </author><guid>http://localhost:1313/post/docs/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%B8%AD%E7%9A%84%E6%9F%A5%E8%AF%A2%E9%94%AE%E5%92%8C%E5%80%BC/</guid>
    <description><![CDATA[查询、键、值（QKV）自注意力机制的核心是查询（Query）、键（Key）和值（Value）这三个向量。理解它们的角色和相互作用对于掌握自注]]></description>
</item><item>
    <title>深度学习中的注意力机制</title>
    <link>http://localhost:1313/post/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</link>
    <pubDate>Tue, 20 Dec 2022 00:00:00 &#43;0000</pubDate><author>
        <name>AriesChen</name>
    </author><guid>http://localhost:1313/post/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</guid>
    <description><![CDATA[深度学习中的注意力机制注意力机制（Attention Mechanism）是深度学习领域的一项关键技术，它使模型能够在处理输入数据时，选择性地]]></description>
</item></channel>
</rss>
